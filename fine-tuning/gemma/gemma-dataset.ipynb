{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset prep for Llama-3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpaca format\n",
    "\n",
    "```\n",
    "{\"instruction\": \"...\", \"input\": \"...\", \"output\": \"...\"}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = \"../../dataset/nl-logql-dataset-03\"\n",
    "dataset = load_dataset(\"sidbin/natural-logql\")\n",
    "\n",
    "df = dataset[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = dataset[\"test\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>application</th>\n",
       "      <th>application_variables</th>\n",
       "      <th>category</th>\n",
       "      <th>label_filter</th>\n",
       "      <th>line_filter</th>\n",
       "      <th>log_category_result</th>\n",
       "      <th>logql_query</th>\n",
       "      <th>metric_category</th>\n",
       "      <th>metric_category_result</th>\n",
       "      <th>query_explanation</th>\n",
       "      <th>query_result</th>\n",
       "      <th>question</th>\n",
       "      <th>row_variables</th>\n",
       "      <th>variables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>openstack</td>\n",
       "      <td>[application]</td>\n",
       "      <td></td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>{'chain_of_thought': 'Analyzing the query, it contains three label filters: `application=\"openstack\"`, `log_file_type=\"nova-compute\"`, `component=\"nova.compute.manager\"`. Additionally, there are three line filters (`|= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\"`, `|= \"Took\"`, `|= \"seconds to spawn the instance on the hypervisor\"`) followed by a regex operation and a line format operation. The presence of three label filters classifies this under multiple log stream selectors, and the presence of more than one line filter classifies it under multiple line filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>{application=\"openstack-us-east\", log_file_type=\"nova-compute\", component=\"nova.compute.manager\"} |= \"3edec1e4-9678-4a3a-a21b-a145a4ee5e61\" |= \"Took\" |= \"seconds to spawn the instance on the hypervisor\" | regexp \"\\\\[instance: (?P&lt;instance_id&gt;[^\\\\]]+)\\\\] Took (?P&lt;spawn_time&gt;\\\\d+\\\\.\\\\d+) seconds to spawn the instance on the hypervisor\" | line_format \"{{.instance_id}} took {{.spawn_time}}\"</td>\n",
       "      <td>None</td>\n",
       "      <td>{'categories': None, 'chain_of_thought': 'The provided query involves log matching and extraction using filters and regexp but does not perform any quantitative aggregations or statistical computations, such as sum, count, or rate on the extracted data. It includes log filters, a regexp pattern for extracting data, and formatting the output using line_format, which suggests manipulation of text rather than numerical computation for metrics. Therefore, there are no metric aggregations present in this query.'}</td>\n",
       "      <td>1\\n{application=\"openstack\", log_file_type=\"nova-compute\", component=\"nova.compute.manager\"}\\nFetch all log lines matching label filters.\\n2\\n&lt;expr&gt; |= `3edec1e4-9678-4a3a-a21b-a145a4ee5e61`\\nReturn log lines that contain string 3edec1e4-9678-4a3a-a21b-a145a4ee5e61.\\n\\n3\\n&lt;expr&gt; |= `Took`\\nReturn log lines that contain string Took.\\n\\n4\\n&lt;expr&gt; |= `seconds to spawn the instance on the hypervisor`\\nReturn log lines that contain string seconds to spawn the instance on the hypervisor.\\n\\n5\\n&lt;expr&gt; | regexp `\\[instance: (?P&lt;instance_id&gt;[^\\]]+)\\] Took (?P&lt;spawn_time&gt;\\d+\\.\\d+) seconds to spawn the instance on the hypervisor`\\nThe regexp parser takes a single parameter | regexp \"&lt;re&gt;\" which is the regular expression using the Golang RE2 syntax. The regular expression must contain a least one named sub-match (e.g (?P&lt;name&gt;re)), each sub-match will extract a different label. The expression matches the structure of a log line. The extracted labels can be used in label filter expressions and ...</td>\n",
       "      <td>3edec1e4-9678-4a3a-a21b-a145a4ee5e61 took 20.58</td>\n",
       "      <td>How long did it take to spawn instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61 on the hypervisor for openstack-us-east?</td>\n",
       "      <td>[instance_id, spawn_time]</td>\n",
       "      <td>[instance_id, time_in_sec]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72</td>\n",
       "      <td>openstack</td>\n",
       "      <td>[application]</td>\n",
       "      <td>Instance Lifecycle</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>{'chain_of_thought': 'This query uses three label filters: `application=\"openstack\"`, `log_file_type=\"nova-compute\"`, and `component=\"nova.compute.manager\"`. Additionally, it contains two line filters: `|= \"cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us\"` and `|= \"Terminating instance\"`. Therefore, the classification includes multiple label filters and multiple line filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>sum(count_over_time({application=\"openstack-asia-pacific\", log_file_type=\"nova-compute\", component=\"nova.compute.manager\"}\\n|= \"cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us\"\\n|= \"Terminating instance\"\\n[1d] offset 1d))</td>\n",
       "      <td>[built_in_range_aggregation, log_range_aggregation]</td>\n",
       "      <td>{'categories': ['built_in_range_aggregation', 'log_range_aggregation'], 'chain_of_thought': 'In this user query, `sum()` and `count_over_time()` are used. `sum()` is recognized as a built-in aggregation operator based on the documentation. `count_over_time()` is identified as a log range aggregation because it aggregates the count of logs over a specified time range (1 day in this case), and includes an `offset` modifier to adjust the timing of the range. Hence, the query combines both built-in range and log range aggregations.'}</td>\n",
       "      <td>1\\n{application=\"openstack-asia-pacific\", log_file_type=\"nova-compute\", component=\"nova.compute.manager\"}\\nFetch all log lines matching label filters.\\n2\\n&lt;expr&gt; |= `cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us`\\nReturn log lines that contain string cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us.\\n\\n3\\n&lt;expr&gt; |= `Terminating instance`\\nReturn log lines that contain string Terminating instance.\\n\\n4\\ncount_over_time(&lt;expr&gt; [1d])\\nThe count of all values in the specified interval. The range vector is set to 1d.\\n\\n5\\nsum(&lt;expr&gt;)\\nCalculates sum over the dimensions.</td>\n",
       "      <td>0</td>\n",
       "      <td>How many instances were terminated on compute node cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us yesterday for application openstack-asia-pacific?</td>\n",
       "      <td>[compute_node, time_in_days]</td>\n",
       "      <td>[compute_node, time_in_days]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>324</td>\n",
       "      <td>hdfs</td>\n",
       "      <td>[application]</td>\n",
       "      <td>Data Transfer and Replication</td>\n",
       "      <td>single log stream selector</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>{'chain_of_thought': 'The query uses a single specific label filter by combining `application=\"hdfs\"` and `component=\"dfs.DataNode$DataTransfer\"` into one query. For line filters, two are applied: one regex match `|~ \"Transmitted block .* to .*\"` and another explicit regex parse match `| regexp \"(?P&lt;source_ip&gt;[\\\\d\\\\.]+):\\\\d+:Transmitted block .* to .*\"`. This indeed constitutes multiple line filters being used in the query.', 'label_filter': 'single log stream selector', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>topk(1,\\n  sum by (source_ip) (\\n    count_over_time(\\n      {application=\"hdfs-us-east\", component=\"dfs.DataNode$DataTransfer\"}\\n      |~ \"Transmitted block .* to .*\"\\n      | regexp \"(?P&lt;source_ip&gt;[\\\\d\\\\.]+):\\\\d+:Transmitted block .* to .*\"\\n      [12h]\\n    )\\n  )\\n)</td>\n",
       "      <td>[built_in_range_aggregation, log_range_aggregation]</td>\n",
       "      <td>{'categories': ['built_in_range_aggregation', 'log_range_aggregation'], 'chain_of_thought': 'The query uses `topk(1, ...)` and `sum by (source_ip)` which are both built-in aggregation operators according to the LogQL documentation. It also uses `count_over_time(...[12h])` which is a function for a log range aggregation. The nested use of `topk` and `sum by` classifies as built-in range aggregation, and `count_over_time` directly refers to log range aggregation.'}</td>\n",
       "      <td>Explanation of the query:\\n\\n1. `{application=\"hdfs-us-east\", component=\"dfs.DataNode$DataTransfer\"}`: This selects all logs from the HDFS application and the DataNode's DataTransfer component, which handles data transfer operations.\\n\\n2. `|~ \"Transmitted block .* to .*\"`: This line filter matches log lines containing successful block transmission events.\\n\\n3. `| regexp \"(?P&lt;source_ip&gt;[\\\\d\\\\.]+):\\\\d+:Transmitted block .* to .*\"`: This extracts the source IP address using a regular expression and assigns it to the label \"source_ip\". Note that we're capturing only the IP part, not the port.\\n\\n4. `[12h]`: This specifies the 12-hour time range as requested in the question.\\n\\n5. `count_over_time(...)`: This counts the occurrences of each unique source_ip over the specified time range.\\n\\n6. `sum by (source_ip) (...)`: This sums the counts for each unique source_ip, giving us the total number of successful block transmissions for each source IP.\\n\\n7. `topk(1, ...)`: This selects the...</td>\n",
       "      <td>\"10.251.65.203\", \"10.251.43.21\"</td>\n",
       "      <td>Which source IP had the highest number of successful block transmissions in the last 12 hours for application hdfs-us-east?</td>\n",
       "      <td>[time_in_hours, topk]</td>\n",
       "      <td>[time_in_hours, topk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>340</td>\n",
       "      <td>hdfs</td>\n",
       "      <td>[application]</td>\n",
       "      <td>Error Analysis</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>{'chain_of_thought': 'The query contains two label filters: `component=\"dfs.DataNode$PacketResponder\"` and `log_level=\"INFO\"`. Additionally, it includes a regular expression line filter `|~ \"PacketResponder .* Exception java.io.IOException: Broken pipe\"` and another regular expression filter `| regexp \"(?P&lt;datanode&gt;\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+:\\\\d+).*PacketResponder .* Exception java.io.IOException: Broken pipe\"`. Given that there are multiple label filters and regular expression filters in the line filter category, the query falls into both multiple label and multiple line filters categories.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>{component=\"dfs.DataNode$PacketResponder\", log_level=\"INFO\", application=\"hdfs-tenant-2\"} |~ \"PacketResponder .* Exception java.io.IOException: Broken pipe\"\\n      | regexp \"(?P&lt;datanode&gt;\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+:\\\\d+).*PacketResponder .* Exception java.io.IOException: Broken pipe\"</td>\n",
       "      <td>None</td>\n",
       "      <td>{'categories': None, 'chain_of_thought': 'The given query filters logs based on a component and a regex pattern that matches specific log entries. It then extracts labels using the regexp operator. However, there are no metric aggregation functions like `sum`, `rate`, or any other functions applied to this log query. This seems to be a plain log query involving log filtering and extraction, with no metric aggregations (log range, unwrapped range, or built-in) directly visible in the given query.'}</td>\n",
       "      <td>Explanation of the query:\\n\\n1. `{component=\"dfs.DataNode$PacketResponder\", log_level=\"INFO\", application=\"hdfs-tenant-2\"}`: This selects all logs from the PacketResponder component with INFO log level for the application 'hdfs-tenant-2'.\\n\\n2. `|~ \"PacketResponder .* Exception java.io.IOException: Broken pipe\"`: This line filter matches log lines containing the 'Broken pipe' exception in PacketResponder.\\n\\n3. `| regexp \"(?P&lt;datanode&gt;\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+:\\\\d+).*PacketResponder .* Exception java.io.IOException: Broken pipe\"`: This extracts the DataNode IP and port using a regular expression and assigns it to the label \"datanode\". We assume the DataNode information is at the beginning of the log line, which is common in distributed system logs.</td>\n",
       "      <td>blk_-4567777441263358151\\nblk_3858821904894294369</td>\n",
       "      <td>Which DataNodes had the highest number of 'Broken pipe' exceptions in PacketResponder threads in the past 12 hours for application 'hdfs-tenant-2'?</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96</td>\n",
       "      <td>openstack</td>\n",
       "      <td>[application]</td>\n",
       "      <td>Error Analysis</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>single line filter</td>\n",
       "      <td>{'chain_of_thought': 'The presented query utilizes a single line filter expressed by '|= \"Bad response code while validating token: 503\"' to examine the logs. Additionally, multiple log stream selectors (label filters) are used, indicated by 'application=\"openstack\"', 'log_level=\"ERROR\"', and 'component=\"keystonemiddleware.auth_token\"'. Thus, the category for line filters is 'single line filter' and for log stream selectors is 'multiple log stream selectors'.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'single line filter'}</td>\n",
       "      <td>sum(count_over_time({application=\"openstack-eu-west\", log_level=\"ERROR\", component=\"keystonemiddleware.auth_token\"}\\n|= \"Bad response code while validating token: 503\"\\n[1h]))</td>\n",
       "      <td>[built_in_range_aggregation, log_range_aggregation]</td>\n",
       "      <td>{'categories': ['built_in_range_aggregation', 'log_range_aggregation'], 'chain_of_thought': 'The query uses `sum()` function and the `count_over_time()` function. `sum()` is identified as a built-in aggregation operator from the documentation. The `count_over_time()` function is characterized as a log range aggregation in the documentation. Thus, the query incorporates both a built-in aggregation and a log-range aggregation.'}</td>\n",
       "      <td>1\\n{application=\"openstack-eu-west\", log_level=\"ERROR\", component=\"keystonemiddleware.auth_token\"}\\nFetch all log lines matching label filters.\\n2\\n&lt;expr&gt; |= `Bad response code while validating token: 503`\\nReturn log lines that contain string Bad response code while validating token: 503.\\n\\n3\\ncount_over_time(&lt;expr&gt; [1h])\\nThe count of all values in the specified interval. The range vector is set to 1h.\\n\\n4\\nsum(&lt;expr&gt;)\\nCalculates sum over the dimensions.</td>\n",
       "      <td>2</td>\n",
       "      <td>How many times did we receive a 503 status code while validating tokens in the last hour for openstack-eu-west?</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>8</td>\n",
       "      <td>openstack</td>\n",
       "      <td>[application]</td>\n",
       "      <td></td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>{'chain_of_thought': 'The user query submits using multiple label filters: `application='openstack'`, `log_file_type='nova-compute'`. There are multiple line filters used sequentially: `|= '3edec1e4-9678-4a3a-a21b-a145a4ee5e61'`, `|= 'Took'`, `|= 'seconds to build instance'`, `| regexp '\\[instance: (?P&lt;instance_id&gt;[^\\]]+)\\d+] Took (?P&lt;build_time&gt;\\d+.\\d+) seconds to build instance'`. By definition, using several different types of line filters suggests it falls under 'multiple line filters'. For labels, using multiple labels as part of the stream selector puts this into the 'multiple log stream selectors' category.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>{application=\"openstack-asia-pacific\", log_file_type=\"nova-compute\"} |= `3edec1e4-9678-4a3a-a21b-a145a4ee5e61` |= `Took` |= `seconds to build instance` | regexp `\\[instance: (?P&lt;instance_id&gt;[^\\]]+)\\] Took (?P&lt;build_time&gt;\\d+\\.\\d+) seconds to build instance` | line_format `{{.build_time}}`</td>\n",
       "      <td>None</td>\n",
       "      <td>{'categories': None, 'chain_of_thought': 'This LogQL query does not contain any aggregation operators like `sum`, `avg`, `max`, `min`, `count`, etc. It appears to involve parsing and restructuring log lines with `regexp` and `line_format` but does not aggregate these logs into metrics. Therefore, it does not fall into the categories of metric aggregation, whether log range, unwrapped range, or built-in range aggregation.'}</td>\n",
       "      <td>1. {application=\"openstack-asia-pacific\", log_file_type=\"nova-compute\"}\\nFetch all log lines matching label filters.\\n2. &lt;expr&gt; |= `3edec1e4-9678-4a3a-a21b-a145a4ee5e61`\\nReturn log lines that contain string 3edec1e4-9678-4a3a-a21b-a145a4ee5e61.\\n\\n3. &lt;expr&gt; |= `Took`\\nReturn log lines that contain string Took.\\n\\n4. &lt;expr&gt; |= `seconds to build instance`\\nReturn log lines that contain string seconds to build instance.\\n\\n5. &lt;expr&gt; | regexp `\\[instance: (?P&lt;instance_id&gt;[^\\]]+)\\] Took (?P&lt;build_time&gt;\\d+\\.\\d+) seconds to build instance`\\nThe regexp parser takes a single parameter | regexp \"&lt;re&gt;\" which is the regular expression using the Golang RE2 syntax. The regular expression must contain a least one named sub-match (e.g (?P&lt;name&gt;re)), each sub-match will extract a different label. The expression matches the structure of a log line. The extracted labels can be used in label filter expressions and used as values for a range aggregation via the unwrap operation.\\n\\n6. &lt;expr&gt; | line_fo...</td>\n",
       "      <td>21.38</td>\n",
       "      <td>What was the total time taken to build instance 3edec1e4-9678-4a3a-a21b-a145a4ee5e61 in openstack-asia-pacific?</td>\n",
       "      <td>[instance_id, build_time]</td>\n",
       "      <td>[instance_id, time_in_sec]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>291</td>\n",
       "      <td>openssh</td>\n",
       "      <td>[application, hostname]</td>\n",
       "      <td>Invalid User Attempts</td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>{'chain_of_thought': 'In this query, there are two label filters set up: `application=\"openssh\"` and `hostname=\"LabSZ\"`. This clearly indicates the use of multiple label filters. Additionally, the query contains two line filters: a simple substring filter `|~ \"Failed password for invalid user\"` and a regular expression filter `| regexp \"Failed password for invalid user (?P&lt;invalid_user&gt;\\S+) from (?P&lt;source_ip&gt;\\d+\\.\\d+\\.\\d+\\.\\d+)\"`. This combination places the query in the category of multiple line filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>sum(\\nsum by (source_ip) (\\n  count_over_time(\\n    {application=\"openssh-eu-west\", hostname=\"LabSZ-eu-west\"}\\n    |~ \"Failed password for invalid user\"\\n    | regexp \"Failed password for invalid user (?P&lt;invalid_user&gt;\\\\S+) from (?P&lt;source_ip&gt;\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+)\"\\n    | __error__=\"\"\\n    [24h]\\n  )\\n) &gt; 200\\n)</td>\n",
       "      <td>[built_in_range_aggregation, log_range_aggregation]</td>\n",
       "      <td>{'categories': ['built_in_range_aggregation', 'log_range_aggregation'], 'chain_of_thought': 'The query employs both `sum()` and `sum by (source_ip)`, which are built-in aggregation operators used in LogQL. Additionally, `count_over_time` specifies a log range aggregation as it uses a duration to aggregate log data over the specified time. Both types of metric aggregations are clearly evident in the query.'}</td>\n",
       "      <td>1\\n{application=\"openssh-eu-west\", hostname=\"LabSZ-eu-west\"}\\nFetch all log lines matching label filters.\\n2\\n&lt;expr&gt; |~ `Failed password for invalid user`\\nReturn log lines that match a RE2 regex pattern. Failed password for invalid user.\\n\\n3\\n&lt;expr&gt; | regexp `Failed password for invalid user (?P&lt;invalid_user&gt;\\S+) from (?P&lt;source_ip&gt;\\d+\\.\\d+\\.\\d+\\.\\d+)`\\nThe regexp parser takes a single parameter | regexp \"&lt;re&gt;\" which is the regular expression using the Golang RE2 syntax. The regular expression must contain a least one named sub-match (e.g (?P&lt;name&gt;re)), each sub-match will extract a different label. The expression matches the structure of a log line. The extracted labels can be used in label filter expressions and used as values for a range aggregation via the unwrap operation.\\n\\n4\\n&lt;expr&gt; | __error__=``\\nFilter out all formatting and parsing errors.\\n\\n5\\ncount_over_time(&lt;expr&gt; [24h])\\nThe count of all values in the specified interval. The range vector is set to 24h.\\n\\n6\\nsum ...</td>\n",
       "      <td>11.5k</td>\n",
       "      <td>How many attempts were made to authenticate with invalid users from each unique source IP in the past 24 hours for openssh-eu-west? (more than 200 attempts)</td>\n",
       "      <td>[num, time_in_hours]</td>\n",
       "      <td>[num, time_in_hours]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>46</td>\n",
       "      <td>openstack</td>\n",
       "      <td>[application]</td>\n",
       "      <td></td>\n",
       "      <td>multiple log stream selectors</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>{'chain_of_thought': 'The query contains two label filters: `application=\"openstack\"` and `log_file_type=\"nova-compute\"`. It includes a line filter `|= \"Active base files:\"` and a regular expression parsing `| regexp \"Active base files: /var/lib/nova/instances/_base/(?P&lt;base_file_id&gt;[a-f0-9]+)\"`. Since there are multiple label filters and both a line filter and a regex parsing expression involved, it should be categorized as having multiple label filters and multiple line filters.', 'label_filter': 'multiple log stream selectors', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>count(sum by (base_file_id, application) \\n(count_over_time({application=\"openstack-eu-west\", log_file_type=\"nova-compute\"}\\n    |= \"Active base files:\"\\n    | regexp \"Active base files: /var/lib/nova/instances/_base/(?P&lt;base_file_id&gt;[a-f0-9]+)\" [30d])))</td>\n",
       "      <td>[built_in_range_aggregation, log_range_aggregation]</td>\n",
       "      <td>{'categories': ['built_in_range_aggregation', 'log_range_aggregation'], 'chain_of_thought': 'The query uses several functions: `count()`, `sum by (base_file_id, application)`, and `count_over_time()`. From LogQL documentation, \n",
       "- `count()` and `sum by (base_file_id, application)` are built-in aggregation operators. \n",
       "- `count_over_time()` is used for log range aggregations as it applies a function to log entries over a specified time range.\n",
       "\n",
       "Therefore, this query combines built-in aggregation operations with log range aggregation to analyze the logs.'}</td>\n",
       "      <td>1\\n{application=\"openstack-eu-west\", log_file_type=\"nova-compute\"}\\nFetch all log lines matching label filters.\\n2\\n&lt;expr&gt; |= `Active base files:`\\nReturn log lines that contain string Active base files:.\\n\\n3\\n&lt;expr&gt; | regexp `Active base files: /var/lib/nova/instances/_base/(?P&lt;base_file_id&gt;[a-f0-9]+)`\\nThe regexp parser takes a single parameter | regexp \"&lt;re&gt;\" which is the regular expression using the Golang RE2 syntax. The regular expression must contain a least one named sub-match (e.g (?P&lt;name&gt;re)), each sub-match will extract a different label. The expression matches the structure of a log line. The extracted labels can be used in label filter expressions and used as values for a range aggregation via the unwrap operation.\\n\\n4\\ncount_over_time(&lt;expr&gt; [30d])\\nThe count of all values in the specified interval. The range vector is set to 30d.\\n\\n5\\nsum by(base_file_id, application) (&lt;expr&gt;)\\nCalculates sum over dimensions while preserving labels base_file_id and application.\\n...</td>\n",
       "      <td>1</td>\n",
       "      <td>How many active base files are currently in use for application openstack-eu-west?</td>\n",
       "      <td>[base_file_id, time_in_days]</td>\n",
       "      <td>[time_in_days, base_file_type: unknown|removable]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>372</td>\n",
       "      <td>hdfs</td>\n",
       "      <td>[application]</td>\n",
       "      <td>Block Management</td>\n",
       "      <td>single log stream selector</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>{'chain_of_thought': 'The query contains a single label filter: `component=\"dfs.DataNode\"`. It includes a line filter with a regular expression `|~ \"Received block .* of size .* from .*\"` and another regex parsing expression `| regexp \"Received block (?P&lt;block_id&gt;blk_[^ ]+) of size\"`. These expressions focus on matching and parsing specific patterns, indicating the presence of multiple line filters in the query.', 'label_filter': 'single log stream selector', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>count(\\n  sum by (block_id) (\\n    count_over_time(\\n      {component=\"dfs.DataNode\", application=\"hdfs-asia-pacific\"}\\n      |~ \"Received block .* of size .* from .*\"\\n      | regexp \"Received block (?P&lt;block_id&gt;blk_[^ ]+) of size\"\\n      [4h]\\n    )\\n  )\\n)</td>\n",
       "      <td>[built_in_range_aggregation, log_range_aggregation]</td>\n",
       "      <td>{'categories': ['built_in_range_aggregation', 'log_range_aggregation'], 'chain_of_thought': 'The query uses `count()`, `sum by (block_id)`, and `count_over_time()`. From the documentation, `count()` and `sum by (block_id)` are built-in aggregation operators, while `count_over_time()` is a type of log range aggregation. Therefore the query covers both built-in range aggregation and log range aggregation.'}</td>\n",
       "      <td>Explanation of the query:\\n\\n1. `{component=\"dfs.DataNode\", application=\"hdfs-asia-pacific\"}`: This selects all logs from the DataNode component for the application hdfs-asia-pacific, which handles block receiving operations.\\n\\n2. `|~ \"Received block .* of size .* from .*\"`: This line filter matches log lines containing the block received event.\\n\\n3. `| regexp \"Received block (?P&lt;block_id&gt;blk_[^ ]+) of size\"`: This extracts the block ID using a regular expression and assigns it to the label \"block_id\".\\n\\n4. `[4h]`: This specifies the 4-hour time range as requested in the question.\\n\\n5. `count_over_time(...)`: This counts the occurrences of each unique block_id over the specified time range.\\n\\n6. `sum by (block_id) (...)`: This sums the counts for each unique block_id, effectively collapsing multiple occurrences of the same block_id into a single entry.\\n\\n7. `count(...)`: This counts the number of unique block_ids after summing, giving us the total number of unique blocks rece...</td>\n",
       "      <td>82.9k</td>\n",
       "      <td>How many unique blocks were received across all DataNodes in the last 4 hours for application hdfs-asia-pacific?</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>403</td>\n",
       "      <td>hdfs</td>\n",
       "      <td>[application]</td>\n",
       "      <td>NameNode Operations</td>\n",
       "      <td>single log stream selector</td>\n",
       "      <td>multiple line filters</td>\n",
       "      <td>{'chain_of_thought': 'This query uses a single label filter: `component=\"dfs.FSNamesystem\"`, to select logs from the specified component. It also includes multiple line filters: the first line filter uses a regular expression to find log lines containing a specific pattern related to stored blocks `|~ \"BLOCK\\* NameSystem\\.addStoredBlock: blockMap updated:.*is added to.*size.*\"`. Following this, another regular expression is applied to extract the size from the log line `| regexp \"BLOCK\\* NameSystem\\.addStoredBlock: blockMap updated:.*is added to.*size (?P&lt;size&gt;[0-9]+)\"`, which further processes the log lines. Thus, the query has a single label filter and multiple line filters.', 'label_filter': 'single log stream selector', 'line_filter': 'multiple line filters'}</td>\n",
       "      <td>sum(\\n  sum_over_time(\\n    {component=\"dfs.FSNamesystem\", application=\"hdfs-south-america\"}\\n    |~ \"BLOCK\\\\* NameSystem\\\\.addStoredBlock: blockMap updated:.*is added to.*size.*\"\\n    | regexp \"BLOCK\\\\* NameSystem\\\\.addStoredBlock: blockMap updated:.*is added to.*size (?P&lt;size&gt;[0-9]+)\"\\n    | unwrap size\\n    [24h]\\n  )\\n)</td>\n",
       "      <td>[built_in_range_aggregation, unwrapped_range_aggregation]</td>\n",
       "      <td>{'categories': ['built_in_range_aggregation', 'unwrapped_range_aggregation'], 'chain_of_thought': 'In the user query, there is a combination of `sum()` and `sum_over_time()`. The `sum()` function is identified as a built-in aggregation operator, focusing on aggregating metrics based on conditions set within its parameters. The `sum_over_time()` function deals with unwrapped range aggregations where it aggregates values over a specified time period from an unwrapped label. In this case, the label `size` is unwrapped and then aggregated over `24h`. These two aggregation types distinguish the use of built-in and unwrapped range aggregations in the query.'}</td>\n",
       "      <td>1. `{component=\"dfs.FSNamesystem\", application=\"hdfs-south-america\"}`: This selects all logs from the FSNamesystem component for the hdfs-south-america application.\\n\\n2. `|~ \"BLOCK\\\\* NameSystem\\\\.addStoredBlock: blockMap updated:.*is added to.*size.*\"`: This line filter matches log lines containing the blockMap update event.\\n\\n3. `| regexp \"BLOCK\\\\* NameSystem\\\\.addStoredBlock: blockMap updated:.*is added to.*size (?P&lt;size&gt;[0-9]+)\"`: This extracts the block size using a regular expression and assigns it to the label \"size\".\\n\\n4. `| unwrap size`: This unwraps the \"size\" label, converting it from a string to a numeric value that can be used in calculations.\\n\\n5. `[24h]`: This specifies the 24-hour time range as requested in the question.\\n\\n6. `sum_over_time(...)`: This sums up all the unwrapped size values over the specified time range.\\n\\n7. `sum(...)`: This calculates the total sum across all instances, giving us the total size of blocks added to the blockMap.\\n\\nThis query e...</td>\n",
       "      <td>16.1 Tri\\n&lt;graph&gt;</td>\n",
       "      <td>What is the total size of blocks added to the blockMap in the last 24 hours for application hdfs-south-america?</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "      <td>[time_in_hours]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  ...                                          variables\n",
       "0     0  ...                         [instance_id, time_in_sec]\n",
       "1    72  ...                       [compute_node, time_in_days]\n",
       "2   324  ...                              [time_in_hours, topk]\n",
       "3   340  ...                                    [time_in_hours]\n",
       "4    96  ...                                    [time_in_hours]\n",
       "..  ...  ...                                                ...\n",
       "80    8  ...                         [instance_id, time_in_sec]\n",
       "81  291  ...                               [num, time_in_hours]\n",
       "82   46  ...  [time_in_days, base_file_type: unknown|removable]\n",
       "83  372  ...                                    [time_in_hours]\n",
       "84  403  ...                                    [time_in_hours]\n",
       "\n",
       "[85 rows x 15 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def prep_for_llama(row: Dict):\n",
    "    instruction = \"You are LogQLLM, a specialized language model trained to convert natural language queries into LogQL (Log Query Language) queries. Your primary function is to interpret user requests expressed in plain English and translate them into valid LogQL syntax.\"\n",
    "    input = row[\"question\"]\n",
    "    output = f\"[QL] {row['logql_query']} [/QL] [EXPLANATION] {row['query_explanation']} [/EXPLANATION]\"\n",
    "    messages = {\"instruction\": instruction, \"input\": input, \"output\": output}\n",
    "    return messages\n",
    "    # return {\"alpaca\": messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f352c9ec034041caac59dc0de6eae0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/339 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "train_data = dataset[\"train\"].map(\n",
    "    prep_for_llama,\n",
    "    num_proc=os.cpu_count(),\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d4f0eb1074466f94bbd0d7b9e8750a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "569714"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.select_columns([\"instruction\", \"input\", \"output\"]).to_json(\"data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
